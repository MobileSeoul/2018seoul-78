{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from PIL import Image as PIL_Image\n",
    "from io import BytesIO as BY\n",
    "import requests\n",
    "import cgi\n",
    "import os\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = requests.get(\"https://hear-seoul.herokuapp.com/api/get-spot-data?key=sadfhlfhqoiuahdfasdfhjq\")\n",
    "load =load.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load['results'][0]['img_src'][0]\n",
    "image = requests.get(img).content\n",
    "name = os.path.basename(img)\n",
    "with open('Data/'+name, 'wb') as f:\n",
    "    f.write(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = load['totalCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "\n",
    "    #이미지 다운로드 및 로드\n",
    "for num in range(count):\n",
    "\n",
    "    img_count = load['results'][num]['img_src']\n",
    "    for img_id in img_count:\n",
    "        img = img_id\n",
    "        image = requests.get(img).content\n",
    "        name = os.path.basename(img)\n",
    "        with open('Data/'+name, 'wb') as f:\n",
    "            f.write(image)\n",
    "            \n",
    "        img = PIL_Image.open(\"Data/\"+name)\n",
    "        img = img.resize((224,224))\n",
    "        img = list(np.asarray(img))\n",
    "        img_t = np.array([img], dtype = np.float32)\n",
    "        dataset.append(img_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "img_label = []\n",
    "\n",
    "\n",
    "    #이미지 로드\n",
    "for num in range(count-1):\n",
    "\n",
    "    img_count = load['results'][num]['img_src']\n",
    "    for img_id in img_count:\n",
    "        img = img_id\n",
    "        name = os.path.basename(img)\n",
    "        img = PIL_Image.open(\"Data/\"+name)\n",
    "        img = img.resize((224,224))\n",
    "        img = list(np.asarray(img))\n",
    "        img_t = np.array([img], dtype = np.float32)\n",
    "        dataset.append(img_t)\n",
    "        img_label.append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count = np.shape(dataset)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STARTING VGG-16\n",
    "\n",
    "\n",
    "# X = SHAPE (NONE, HEIGHT, WIDTH, CHANNELS)\n",
    "X = tf.placeholder(shape = (None, 224,224, 3), dtype = tf.float32)\n",
    "\n",
    "\n",
    "#VGG -16 START\n",
    "conv1 = tf.layers.conv2d(X, filters = 64, kernel_size = 3, strides = [1,1], padding=\"SAME\",\n",
    "                       activation = tf.nn.relu)\n",
    "conv2 = tf.layers.conv2d(conv1, filters = 64, kernel_size = 3, strides = [1,1], padding=\"SAME\",\n",
    "                       activation = tf.nn.relu)\n",
    "\n",
    "pool1 = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides = [1,2,2,1], padding=\"VALID\")\n",
    "\n",
    "\n",
    "#FIRST END\n",
    "\n",
    "\n",
    "conv3 = tf.layers.conv2d(pool1, filters = 128, kernel_size = 3, strides = [1,1], padding=\"SAME\",\n",
    "                       activation = tf.nn.relu)\n",
    "conv4 = tf.layers.conv2d(conv3, filters = 128, kernel_size = 3, strides = [1,1], padding=\"SAME\",\n",
    "                       activation = tf.nn.relu)\n",
    "\n",
    "pool2 = tf.nn.max_pool(conv4, ksize=[1,2,2,1], strides = [1,2,2,1], padding=\"VALID\")\n",
    "\n",
    "\n",
    "#SECOND END\n",
    "\n",
    "conv5 = tf.layers.conv2d(pool2, filters = 256, kernel_size = 3, strides = [1,1], padding=\"SAME\",\n",
    "                       activation = tf.nn.relu)\n",
    "conv6 = tf.layers.conv2d(conv5, filters = 256, kernel_size = 3, strides = [1,1], padding=\"SAME\",\n",
    "                       activation = tf.nn.relu)\n",
    "\n",
    "pool3 = tf.nn.max_pool(conv6, ksize=[1,2,2,1], strides = [1,2,2,1], padding=\"VALID\")\n",
    "\n",
    "\n",
    "#THIRD END\n",
    "\n",
    "\n",
    "conv7 = tf.layers.conv2d(pool3, filters = 512, kernel_size = 3, strides = [1,1], padding=\"SAME\",\n",
    "                       activation = tf.nn.relu)\n",
    "conv8 = tf.layers.conv2d(conv7, filters = 512, kernel_size = 3, strides = [1,1], padding=\"SAME\",\n",
    "                       activation = tf.nn.relu)\n",
    "conv9 = tf.layers.conv2d(conv8, filters = 512, kernel_size = 3, strides = [1,1], padding=\"SAME\",\n",
    "                       activation = tf.nn.relu)\n",
    "\n",
    "pool4 = tf.nn.max_pool(conv9, ksize=[1,2,2,1], strides = [1,2,2,1], padding=\"VALID\")\n",
    "\n",
    "\n",
    "#FOURTH END\n",
    "\n",
    "\n",
    "\n",
    "conv10 = tf.layers.conv2d(pool4, filters = 512, kernel_size = 3, strides = [1,1], padding=\"SAME\",\n",
    "                       activation = tf.nn.relu)\n",
    "conv11 = tf.layers.conv2d(conv10, filters = 512, kernel_size = 3, strides = [1,1], padding=\"SAME\",\n",
    "                       activation = tf.nn.relu)\n",
    "conv12 = tf.layers.conv2d(conv11, filters = 512, kernel_size = 3, strides = [1,1], padding=\"SAME\",\n",
    "                       activation = tf.nn.relu)\n",
    "\n",
    "\n",
    "#VGG -16 END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "#saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "feature = []\n",
    "\n",
    "#VGG-16 RUN\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    \n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        init.run()\n",
    "        for id in range(np.shape(dataset)[0]):\n",
    "            features = sess.run(conv12, feed_dict={X: dataset[id]})\n",
    "            \n",
    "            feature.append(features)\n",
    "            #saver.save(sess, 'VGG_model/VGG.module')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_array= np.asarray(feature, dtype=np.float32)\n",
    "#X_array=feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#PCA\n",
    "#충분한 분산이 될때까지 더해야 할 차원 수 선택\n",
    "#sess = tf.Session()\n",
    "#with sess.as_default():\n",
    "#    X_train=conv12.eval()\n",
    "#feats = normalize(features)\n",
    "X_reduced=[]\n",
    "dimention_count=2\n",
    "for i in range(image_count):\n",
    "    tmp=X_array[i][0]\n",
    "    X=tmp.reshape(14,7168)# 14*14*512  => 14*7168  2dimention input require\n",
    "    pca=PCA(n_components=dimention_count)\n",
    "    X_reduced.append(pca.fit_transform(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216, 14, 2)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dArray=np.asarray(X_reduced, dtype=np.float32) #(image_count,14,1)\n",
    "dArray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dArray=np.asarray(X_reduced, dtype=np.float32) #(image_count,14,1)\n",
    "X_reduced2=dArray.reshape(image_count,14*dimention_count)\n",
    "#X_reduced2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216, 28)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduced2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#kmeans\n",
    "# k개수 지정 k=(sqrt n/2) n은 데이터의 개수\n",
    "\n",
    "    # 모든 데이터를 상수 텐서로 옮김\n",
    "\n",
    "#k=np.sqrt(np.size(X_reduced2)/2).astype('int')\n",
    "# change to k=5\n",
    "k=5\n",
    "vectors = tf.constant(X_reduced2)\n",
    "\n",
    "centroides = tf.Variable(tf.slice(tf.random_shuffle(vectors),[0,0],[k,-1]))\n",
    "\n",
    "expanded_vectors = tf.expand_dims(vectors, 0)\n",
    "expanded_centroides = tf.expand_dims(centroides, 1)\n",
    "\n",
    "diff = tf.subtract(expanded_vectors, expanded_centroides)\n",
    "sqr = tf.square(diff)\n",
    "distances = tf.reduce_sum(sqr, 2)\n",
    "assignments = tf.argmin(distances,0)\n",
    "\n",
    "means = tf.concat([tf.reduce_mean(tf.gather(vectors, tf.reshape( tf.where(tf.equal(assignments, c)) ,[1, -1])), reduction_indices=[1]) for c in range(k)],0)\n",
    "\n",
    "update_centroides = tf.assign(centroides, means)\n",
    "\n",
    "\n",
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "for step in range(100):\n",
    "    _, centroid_values, assignment_values = sess.run([update_centroides, centroides, assignments])\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \n",
    "#knn\n",
    "\n",
    "knn = KNeighborsClassifier(k)\n",
    "knn.fit(X_reduced2, assignment_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216, 28)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduced2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_reduced2.shape\n",
    "temp=X_reduced2#[1].reshape(1,14)\n",
    "y_pred = knn.predict(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 2,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 11,\n",
       " 12,\n",
       " 15,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 43,\n",
       " 44,\n",
       " 58,\n",
       " 66,\n",
       " 74,\n",
       " 75,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 84,\n",
       " 90,\n",
       " 91,\n",
       " 100,\n",
       " 106,\n",
       " 108,\n",
       " 119,\n",
       " 120,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 130,\n",
       " 136,\n",
       " 144,\n",
       " 154,\n",
       " 167,\n",
       " 171,\n",
       " 174,\n",
       " 175,\n",
       " 177,\n",
       " 178,\n",
       " 180,\n",
       " 181,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 191,\n",
       " 195,\n",
       " 212,\n",
       " 214]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# 에측 값과 같은 라벨을 가진, 인덱스를 저장\n",
    "\n",
    "#예측할 값\n",
    "Test = 0\n",
    "\n",
    "#라벨\n",
    "Target = y_pred[Test]\n",
    "\n",
    "#훈련된 라벨\n",
    "True_label = assignment_values.tolist()\n",
    "\n",
    "#같은 라벨을 가지고 있는 값의 인덱스를 찾아 저장\n",
    "Target_label = [i for i,x in enumerate(True_label) if x == Target]\n",
    "\n",
    "Target_label\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#해당 값과 예측된 값의 지도상의 좌표를 가져와, 유클리디언 거리를 측정\n",
    "\n",
    "#지구의 반지름\n",
    "R = 6371\n",
    "\n",
    "#거리 저장 변수\n",
    "PD = []\n",
    "\n",
    "#테스트 데이터 GPS정보\n",
    "Test_la = radians(load['results'][Test]['location']['latitude'])\n",
    "Test_lo = radians(load['results'][Test]['location']['longitude'])\n",
    "\n",
    "#각 같은 LABEL별로 유클리디언 거리 측정 후 저장\n",
    "for label in Target_label:\n",
    "    num = img_label[label]\n",
    "    Train_la = radians(load['results'][num]['location']['latitude'])\n",
    "    Train_lo = radians(load['results'][num]['location']['longitude'])\n",
    "    \n",
    "    dlon = Train_lo - Test_lo\n",
    "    dlat = Train_la - Test_la\n",
    "    \n",
    "    a = sin(dlat / 2) **2 + cos(Test_la)*cos(Train_la)*sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "    \n",
    "    distance = R * c\n",
    "    \n",
    "    PD.append(distance)\n",
    "    \n",
    "#가장 작은 거리 찾기\n",
    "ANS = PD.index(min(PD))\n",
    "ANS = img_label[Target_label[ANS]]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Result = {\"recommand\" : \n",
    "          [\n",
    "              {\"ID\" : load['results'][Test]['objectId']},\n",
    "              {\"Suggest\" : load['results'][ANS]['objectId']}\n",
    "          ]\n",
    "         }\n",
    "json.dumps(Result)\n",
    "\n",
    "with open('Result.json','w',encoding=\"utf-8\") as mkfil:\n",
    "    json.dump(Result, mkfil)\n",
    "    \n",
    "for i in range(count):\n",
    "    payload={\"id\":load['results'][i]['id'],\n",
    "        \"suggest\":load['results'][i]['id']}\n",
    "    requests.get('https://hear-seoul.herokuapp.com/api/suggestion-upload', params=payload)\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에측 값과 같은 라벨을 가진, 인덱스를 저장\n",
    "for i in range(count):\n",
    "    #예측할 값\n",
    "    Test = i\n",
    "\n",
    "    #라벨\n",
    "    Target = y_pred[Test]\n",
    "\n",
    "    #훈련된 라벨\n",
    "    True_label = assignment_values.tolist()\n",
    "\n",
    "    #같은 라벨을 가지고 있는 값의 인덱스를 찾아 저장\n",
    "    Target_label = [i for i,x in enumerate(True_label) if x == Target]\n",
    "\n",
    "    #Target_label\n",
    "    #해당 값과 예측된 값의 지도상의 좌표를 가져와, 유클리디언 거리를 측정\n",
    "\n",
    "    #지구의 반지름\n",
    "    R = 6371\n",
    "\n",
    "    #거리 저장 변수\n",
    "    PD = []\n",
    "\n",
    "    #테스트 데이터 GPS정보\n",
    "    Test_la = radians(load['results'][Test]['location']['latitude'])\n",
    "    Test_lo = radians(load['results'][Test]['location']['longitude'])\n",
    "\n",
    "    #각 같은 LABEL별로 유클리디언 거리 측정 후 저장\n",
    "    for label in Target_label:\n",
    "        num = img_label[label]\n",
    "        Train_la = radians(load['results'][num]['location']['latitude'])\n",
    "        Train_lo = radians(load['results'][num]['location']['longitude'])\n",
    "\n",
    "        dlon = Train_lo - Test_lo\n",
    "        dlat = Train_la - Test_la\n",
    "\n",
    "        a = sin(dlat / 2) **2 + cos(Test_la)*cos(Train_la)*sin(dlon/2)**2\n",
    "        c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "\n",
    "        distance = R * c\n",
    "\n",
    "        PD.append(distance)\n",
    "\n",
    "    #가장 작은 거리 찾기\n",
    "    ANS = PD.index(min(PD))\n",
    "    ANS = img_label[Target_label[ANS]]\n",
    "\n",
    "'''\n",
    "    Result = {\"recommand\" : \n",
    "              [\n",
    "                  {\"ID\" : load['results'][Test]['objectId']},\n",
    "                  {\"Suggest\" : load['results'][ANS]['objectId']}\n",
    "              ]\n",
    "             }\n",
    "    json.dumps(Result)\n",
    "\n",
    "    with open('Result.json','w',encoding=\"utf-8\") as mkfil:\n",
    "        json.dump(Result, mkfil)\n",
    "  '''  \n",
    "    payload={\"id\":load['results'][i]['id'],\n",
    "        \"suggest\":load['results'][ANS]['id']}\n",
    "    requests.get('https://hear-seoul.herokuapp.com/api/suggestion-upload?key=sadfhlfhqoiuahdfasdfhjq', params=payload)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
